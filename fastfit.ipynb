{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets fast-fit transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer\n",
    "from fastfit import FastFitTrainer, sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebca6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0282077",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLAB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    base_dir = 'drive/MyDrive/Github/NLPSharedTask'\n",
    "else:\n",
    "    base_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d997f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_train_test_split(\n",
    "    data: pd.DataFrame,\n",
    "    label_col: str = 'label',\n",
    "    test_size: float = 0.3,\n",
    "    random_state: int | None = None\n",
    ") -> dict:\n",
    "    \"\"\"Creates train-test split that makes sure that at least two abstracts for each id are in the test set.\"\"\"\n",
    "\n",
    "    abstract_data = data[data.is_abstract == 1]\n",
    "\n",
    "    # Randomly sample 2 abstracts per sdg group\n",
    "    test_a = abstract_data.groupby(label_col).sample(n=1, random_state=random_state)\n",
    "\n",
    "    # Remove the entries already in the test set from the rest of the data\n",
    "    data = data[~data.index.isin(test_a.index)].copy()\n",
    "\n",
    "    # Split the remaining data into train and test\n",
    "    train, test_b = train_test_split(data, test_size=test_size, random_state=random_state, stratify=data[label_col])\n",
    "\n",
    "    # Concatenate both test sets and shuffle them again\n",
    "    test = pd.concat([test_a, test_b]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data_with_null_with_synth.csv')\n",
    "\n",
    "df['label'] = df.label.astype(str)\n",
    "\n",
    "train_df, temp_df = rule_based_train_test_split(df, random_state=42, test_size=0.3)\n",
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Select only the 'text' and 'sdg' columns for the final datasets\n",
    "train_df = train_df[['text_clean', 'label']]\n",
    "val_df = val_df[['text_clean', 'label']]\n",
    "test_df = test_df[['text_clean', 'label']]\n",
    "\n",
    "# Saving the datasets to CSV files\n",
    "train_df.to_csv(f'{base_dir}/train_data.csv', index=False)\n",
    "val_df.to_csv(f'{base_dir}/val_data.csv', index=False)\n",
    "test_df.to_csv(f'{base_dir}/test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastFit tokenizes the labels, does not like integer labels :(\n",
    "    # transforming the label to text ones. wasted too much time on this for no reason probably.\n",
    "import csv\n",
    "\n",
    "NUMBER_MAPPINGS={\n",
    "    0:\"zero\",\n",
    "    1:\"one\",\n",
    "    2:\"two\",\n",
    "    3:\"three\",\n",
    "    4:\"four\",\n",
    "    5:\"five\",\n",
    "    6:\"six\",\n",
    "    7:\"seven\",\n",
    "    8:\"eight\",\n",
    "    9:\"nine\",\n",
    "    10:\"ten\",\n",
    "    11:\"eleven\",\n",
    "    12:\"twelve\",\n",
    "    13:\"thirteen\",\n",
    "    14:\"fourteen\",\n",
    "    15:\"fifteen\",\n",
    "    16:\"sixteen\",\n",
    "    17:\"seventeen\"\n",
    "}\n",
    "\n",
    "def convert_integers_to_strings(file_path, output_path):\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [row for row in reader]\n",
    "\n",
    "    # Change integers in the second column to strings\n",
    "    for row in data:\n",
    "        if row and len(row) > 1 and row[1] != 'sdg':\n",
    "            try:\n",
    "                # Convert the second column to string if it's an integer\n",
    "                row[1] = NUMBER_MAPPINGS[int(row[1])]\n",
    "            except ValueError:\n",
    "                # If it's not an integer, do nothing\n",
    "                pass\n",
    "\n",
    "    # Write the updated data to a new CSV file\n",
    "    with open(output_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "for path in ['train_data.csv', 'val_data.csv', 'test_data.csv']:\n",
    "    convert_integers_to_strings(path,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a674ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset from the CSV files\n",
    "dataset = load_dataset('csv', data_files={\n",
    "    'train': f'{base_dir}/train_data.csv',\n",
    "    'validation': f'{base_dir}/val_data.csv',\n",
    "    'test': f'{base_dir}/test_data.csv'\n",
    "})\n",
    "\n",
    "\n",
    "# Initialize the FastFit trainer with correct column names and paths\n",
    "trainer = FastFitTrainer(\n",
    "    model_name_or_path=\"allenai/scibert_scivocab_cased\",\n",
    "    label_column_name=\"label\",\n",
    "    text_column_name=\"text_clean\",\n",
    "    num_train_epochs=40,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_text_length=128,\n",
    "    dataloader_drop_last=False,\n",
    "    num_repeats=4,\n",
    "    optim=\"adafactor\",\n",
    "    clf_loss_factor=0.1,\n",
    "    fp16=True,\n",
    "    dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c50d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e36669",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"Accuracy: {:.1f}%\".format(results[\"eval_accuracy\"] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"drive/MyDrive/scibert_all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
