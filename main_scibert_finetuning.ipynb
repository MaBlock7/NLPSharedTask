{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":104412,"status":"ok","timestamp":1714332799214,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"eKi6pN3l-Lq-"},"outputs":[],"source":["%%capture\n","! pip install datasets fast-fit transformers accelerate\n","! pip install evaluate\n","! pip install langdetect"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19068,"status":"ok","timestamp":1714332919871,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"Ldwr5rbHe4E2","outputId":"82686e6c-9171-4894-b397-67610163c2a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714332919871,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"dueQezvpIMhK"},"outputs":[],"source":["USE_COLAB = True"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":20486,"status":"ok","timestamp":1714332940354,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"jyvXFEa79bgv"},"outputs":[],"source":["import os\n","import evaluate\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from sklearn.model_selection import train_test_split\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer,\n","    pipeline,\n",")\n","from datasets import Dataset\n","\n","if USE_COLAB:\n","    from drive.MyDrive.Github.NLPSharedTask.essentials.config import ABSTRACTS\n","    from drive.MyDrive.Github.NLPSharedTask.essentials.data_functions import read_data\n","else:\n","    from essentials.config import ABSTRACTS\n","    from essentials.data_functions import read_data"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1405,"status":"ok","timestamp":1714332969836,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"ibUlLPvB9bg0","outputId":"885c8113-cef1-4ea1-da19-642303e3442b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Select device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define model\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    'allenai/scibert_scivocab_uncased',\n","    num_labels=18,\n","    return_dict=True)\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1714332973566,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"FrB7ea66IfWP"},"outputs":[],"source":["if USE_COLAB:\n","    base_dir = 'drive/MyDrive/Github/NLPSharedTask'\n","else:\n","    base_dir = ''"]},{"cell_type":"markdown","metadata":{"id":"egyf8gv99bg1"},"source":["# LOAD DATA"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"RMpjHR6_E8nz","executionInfo":{"status":"ok","timestamp":1714333094916,"user_tz":-120,"elapsed":1822,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"}}},"outputs":[],"source":["df = pd.read_csv(os.path.join(base_dir, 'cleaned_data_with_null_with_weakly_labeled_with_synth.csv'))"]},{"cell_type":"code","source":["def balance_classes(df, target_col, balance_level=0.5):\n","    \"\"\"\n","    Balance the class distribution in a DataFrame to reduce class imbalances.\n","\n","    Args:\n","    df (pd.DataFrame): The DataFrame containing the data.\n","    target_col (str): The name of the column that contains the target classes.\n","    balance_level (float, optional): A float between 0 and 1 where 0 means no balancing and 1 means full balancing. Default is 0.5.\n","\n","    Returns:\n","    pd.DataFrame: A DataFrame with adjusted class distribution.\n","    \"\"\"\n","    if not (0 <= balance_level <= 1):\n","        raise ValueError(\"Balance level must be between 0 and 1.\")\n","\n","    # Calculate class counts and determine the maximum number we want for each class\n","    class_counts = df[target_col].value_counts()\n","    max_size = class_counts.min() + (class_counts.max() - class_counts.min()) * balance_level\n","\n","    # Sample from each class\n","    def resample_class(group):\n","        n = min(len(group), int(max_size))\n","        return group.sample(n, replace=False)\n","\n","    # Group by the target column and apply sampling\n","    balanced_df = df.groupby(target_col).apply(resample_class).reset_index(drop=True)\n","\n","    return balanced_df"],"metadata":{"id":"R0Xy8O_qN897"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFlByIjv9bg7"},"source":["# CREATE TRAIN/TEST SPLIT"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":253,"status":"ok","timestamp":1714333289489,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"5Ob8UBoE9bg7"},"outputs":[],"source":["def tokenize_text(texts):\n","    return tokenizer(texts, truncation=True, max_length=256, return_tensors=None)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1714333291067,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"tWVyQQoF9bg8"},"outputs":[],"source":["def rule_based_train_test_split(\n","    data: pd.DataFrame,\n","    label_col: str = 'label',\n","    test_size: float = 0.3,\n","    random_state: int | None = None\n",") -> dict:\n","    \"\"\"Creates train-test split that makes sure that at least two abstracts for each id are in the test set.\"\"\"\n","\n","    abstract_data = data[data.is_abstract == 1]\n","\n","    # Randomly sample 2 abstracts per sdg group\n","    test_a = abstract_data.groupby(label_col).sample(n=1, random_state=random_state)\n","\n","    # Remove the entries already in the test set from the rest of the data\n","    data = data[~data.index.isin(test_a.index)].copy()\n","\n","    # Split the remaining data into train and test\n","    train, test_b = train_test_split(data, test_size=test_size, random_state=random_state, stratify=data[label_col])\n","\n","    # Concatenate both test sets and shuffle them again\n","    test = pd.concat([test_a, test_b]).sample(frac=1).reset_index(drop=True)\n","\n","    return train, test"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"rvvG6Y_s9bg7","executionInfo":{"status":"ok","timestamp":1714333325157,"user_tz":-120,"elapsed":22589,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"}}},"outputs":[],"source":["# Apply huggingface tokenizer\n","tokenized_output = tokenize_text(df['text_clean'].to_list())"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6473,"status":"ok","timestamp":1714333740077,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"esG9eijS9bg8"},"outputs":[],"source":["df_tokenized = pd.DataFrame({\n","    'raw_text': df['text_clean'].tolist(),\n","    'input_ids': list(tokenized_output['input_ids']),\n","    'attention_mask': list(tokenized_output['attention_mask']),\n","    'token_type_ids': list(tokenized_output.get('token_type_ids', [[]]*len(df))),\n","    'label': df['label'].tolist(),\n","    'is_abstract': df['is_abstract'].to_list()\n","})\n","\n","train_df, test_df = rule_based_train_test_split(df_tokenized, random_state=42)\n","\n","train_df.to_csv(os.path.join(base_dir, 'train_df.csv'))\n","test_df.to_csv(os.path.join(base_dir, 'test_df.csv'))\n","\n","train_dataset = Dataset.from_pandas(train_df[['input_ids', 'attention_mask', 'token_type_ids', 'label']])\n","test_dataset = Dataset.from_pandas(test_df[['input_ids', 'attention_mask', 'token_type_ids', 'label']])"]},{"cell_type":"markdown","metadata":{"id":"swC39Frp9bg8"},"source":["# FINE-TUNING"]},{"cell_type":"markdown","metadata":{"id":"5FAphzQy9bg8"},"source":["For training, use the suggested values from the paper:\n","\n","In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":195,"status":"ok","timestamp":1714333745110,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"vwLvkAo99bg8"},"outputs":[],"source":["# Multiple class prediction (one prediction)\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = evaluate.load(\"accuracy\")\n","    f1 = evaluate.load(\"f1\")\n","    return {\n","        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n","        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n","    }"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"K2r4rFcM9bg8","executionInfo":{"status":"ok","timestamp":1714333749154,"user_tz":-120,"elapsed":201,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"}}},"outputs":[],"source":["# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(base_dir, 'models/results'),\n","    num_train_epochs=2,  # As best setting suggested 2 or 4\n","    warmup_steps=500,  # Slanted triangular schedule start\n","    learning_rate=2e-5,  # Best learning rate as suggested in the paper\n","    weight_decay=0.01,\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    lr_scheduler_type='linear',  # Corresponds to linear warmup followed by linear decay\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":759,"status":"ok","timestamp":1714333753289,"user":{"displayName":"Manuel Bolz","userId":"08740830567431077125"},"user_tz":-120},"id":"lFws_yMioF7Y"},"outputs":[],"source":["# Data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# Adam Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","\n","# Multiple class Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    optimizers=(optimizer, None)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"IwmLDp5i9bg9","outputId":"16fa8652-0700-45fa-dc2d-55e049711655"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='588' max='10652' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  588/10652 01:53 < 32:31, 5.16 it/s, Epoch 0.11/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["# Start training\n","torch.cuda.empty_cache()\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsEVQXLK9bg9"},"outputs":[],"source":["# Saving the model\n","model_path = os.path.join(base_dir, 'models/scibert_model_with_null_with_synth_with_weakly_labeled')\n","trainer.save_model(model_path)\n","\n","# Saving the tokenizer associated with the model\n","tokenizer.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Kb3oeAN9bg9"},"outputs":[],"source":["# Load the trained model\n","model = AutoModelForSequenceClassification.from_pretrained(os.path.join(base_dir, 'models/scibert_model_base'))\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(os.path.join(base_dir, 'models/scibert_model_base'))\n","\n","# Create a prediction pipeline\n","nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbp4iboQ9bg-"},"outputs":[],"source":["nlp('evolutionary dynamic structural genetic variation lineage hybrid origin not well explored although structural mutation may increase controlled hybrid cross therefore tested whether structural variant accumulate fish recent hybrid origin invasive cottus relative parental specie cottus rhenanus cottus perifretum variation exon gene assessed using comparative genome hybridization array twelve gene showed significantly higher copy number invasive cottus compared parent coincided increased expression three gene related vision detoxification muscle development suggesting possible gene dosage effect copy number increase putative transposon assessed comparative mapping genomic dna read de novo assembly repetitive element contrast exon copy number increase repetitive element common invasive cottus whereas decrease rare among increased repetitive element occurred higher number perifretum compared rhenanus abundant rhenanus implies biased mutational process amplifies genetic material one ancestor ass frequency de novo mutation hybridization screened f offspring parental specie change five candidate locus found no evidence new structural variant indicating rare detected given sampling scheme instead must accumulated generation observed controlled cross')"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}